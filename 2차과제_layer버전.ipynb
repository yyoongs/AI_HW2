{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.loadtxt('mnist.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(csv_dataset): # i는 0~100 중의 하나의 수로 train_set의 비율을 나타낸다. ex) 70 => train_set 70% test_set 30%\n",
    "    #코드 작성\n",
    "    \n",
    "    train_T = csv_dataset[0:8000,0]\n",
    "    train_X = csv_dataset[0:8000,1:785]\n",
    "    test_T = csv_dataset[8000:10000,0]\n",
    "    test_X = csv_dataset[8000:10000,1:785]\n",
    "#   csv_dataset을 적절하게 슬라이싱 하여 각각의 dataset으로 설정\n",
    "#   train_T의 경우 0부터 8000까지 인덱스 0 -> shape (8000)\n",
    "#   train_X의 경우 0부터 8000까지 인덱스 1부터 785 -> shape (8000,784)\n",
    "#   test_T의 경우 8000부터 10000까지 인덱스 0 -> shape (2000)\n",
    "#   test_X의 경우 8000부터 10000까지 인덱스 1부터 785 -> shape (2000,784)\n",
    "    \n",
    "    train_X = train_X / 256\n",
    "    test_X = test_X / 256\n",
    "#   train_X와 test_X의 normalization\n",
    "    \n",
    "    return train_X, train_T, test_X, test_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(T): # T is data의 label\n",
    "    #코드 작성\n",
    "    one_hot_label = np.zeros((T.shape[0],10))\n",
    "#   T의 batch 크기 * 10 인 원소가 0인 넘파이 배열 생성\n",
    "    n = 0    \n",
    "    for i in T:\n",
    "        one_hot_label[n][int(i)] = 1\n",
    "        n += 1\n",
    "        \n",
    "#   T에 들어있는 원소값의 자리를 0에서 1로 바꿔준다\n",
    "#   batch 크기만큼 반복 (train_T는 8000번, test_T는 2000번 반복)\n",
    "\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix): # 제공.\n",
    "\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp - np.max(temp, axis=1, keepdims=True)\n",
    "        y_predict = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return y_predict\n",
    "    temp = ScoreMatrix - np.max(ScoreMatrix, axis=0)\n",
    "    expX = np.exp(temp)\n",
    "    y_predict = expX / np.sum(expX)\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParam_He(neuronlist):\n",
    "\n",
    "    np.random.seed(1) # seed값 고정을 통해 input이 같으면 언제나 같은 Weight와 bias를 출력하기 위한 함수\n",
    "    #코드 작성\n",
    "    #neuronlist = 784,60,30,10\n",
    "    W1 = np.random.randn(neuronlist[0],neuronlist[1]) / np.sqrt(neuronlist[0]/2)\n",
    "    W2 = np.random.randn(neuronlist[1],neuronlist[2]) / np.sqrt(neuronlist[1]/2)\n",
    "    W3 = np.random.randn(neuronlist[2],neuronlist[3]) / np.sqrt(neuronlist[2]/2)\n",
    "#   he initialization 방법으로 W를 초기화\n",
    "    \n",
    "    b1 = np.zeros(neuronlist[1])\n",
    "    b2 = np.zeros(neuronlist[2])\n",
    "    b3 = np.zeros(neuronlist[3])\n",
    "#   bias는 0으로 초기화\n",
    "\n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearLayer:\n",
    "    def __init__(self, W, b):\n",
    "        #backward에 필요한 X, W, b 값 저장 + dW, db값 받아오기\n",
    "        self.X = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        Z = np.dot(x, self.W) + self.b\n",
    "#       Z(결과값) = 입력받은 x값에 weight를 곱하고 bias를 더하여 계산한다\n",
    "        #내적연산을 통한 Z값 계산\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "#         dz는 dout\n",
    "#         dx는 input x의 dx\n",
    "        dx = np.dot(dZ, self.W.T)\n",
    "#       dx는 dout값인 dZ에 현재 linearlayer의 weight.T를 곱하여 구한다\n",
    "        self.dW = np.dot(self.X.T,dZ)\n",
    "#       dW는 dX와 마찬가지로 현재 linearlayer의 X.T에 dout인 dZ를 곱하여 구한다\n",
    "        self.db = np.sum(dZ,axis=0)\n",
    "#       db는 덧셈 연산이기 때문에 dout인 dZ를 axis=0인 방향으로 더하여 계산한다\n",
    "        #백워드 함수\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropLayer:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.mask = None\n",
    "#         이전 layer에서 들어온 activation 값을 저장하는 로컬 변수 x\n",
    "#         dropout layer의 마스크를 저장하는 로컬 변수 mask (forward 시 mask가 생성되고 backward 실행시 필요)\n",
    "    \n",
    "    def forward(self, x, k):\n",
    "        self.X = x\n",
    "#       이전 layer에서 들어온 값 저장\n",
    "        u1 = np.random.rand(*x.shape) > k\n",
    "#       x의 shape를 가진 random 넘파이배열 생성후 kill_n_h 와 비교후 T/F의 값을 가진 마스크 생성  \n",
    "        self.mask = u1\n",
    "#       마스크 저장\n",
    "        Z = x * self.mask\n",
    "#       이전 layer에서 들어온 activation 값에 mask를 적용한 결과를 리턴\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "#         dz는 dout\n",
    "#         dx는 input x의 dx\n",
    "        dx = dZ * self.mask\n",
    "#       backward 진행시에도 layer에 저장된 mask를 적용한 결과를 리턴\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None # 백워드 시 사용할 로컬 변수\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 +np.exp(-x))\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "        Activation = Z * self.sigmoid(Z)\n",
    "        #수식에 따른 forward 함수 작성\n",
    "        return Activation\n",
    "    \n",
    "#     def swish(x):\n",
    "#     return x*sig(x)\n",
    "\n",
    "# def dswish(x):\n",
    "#     return swish(x) + sig(x)*(1-swish(x))\n",
    "\n",
    "    def backward(self, dActivation):\n",
    "        \n",
    "#       dActivation이 dout\n",
    "#       dZ가 dx\n",
    "#       dZ = (self.Z + self.sigmoid(dActivation) * (1-self.Z)) * dActivation\n",
    "        dZ = (self.forward(self.Z) + self.sigmoid(self.Z)*(1-self.forward(self.Z)))*dActivation\n",
    "        #수식에 따른 backward 함수 작성\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(): # 제공\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.softmaxScore = None\n",
    "        self.label = None\n",
    "        \n",
    "    def forward(self, score, one_hot_label):\n",
    "        \n",
    "        batch_size = one_hot_label.shape[0]\n",
    "        self.label = one_hot_label\n",
    "        self.softmaxScore = Softmax(score)\n",
    "        self.loss = -np.sum(self.label * np.log(self.softmaxScore + 1e-20)) / batch_size\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.label.shape[0]\n",
    "        dx = (self.softmaxScore - self.label) / batch_size\n",
    "        \n",
    "        return dx\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet :\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def scoreFunction(self, x):\n",
    "        \n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            # 한 줄이 best\n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "\n",
    "        score = self.scoreFunction(x)\n",
    "        return self.lastLayer.forward(score, label)\n",
    "#     리턴은 loss\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy  = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backpropagation(self, x, label):\n",
    "        dx = self.lastLayer.backward()\n",
    "        self.layers = OrderedDict(reversed(list(self.layers.items())))\n",
    "        for backlayer in self.layers.values():\n",
    "            dx = backlayer.backward(dx)\n",
    "        #백워드 함수 작성 스코어펑션을 참고하세요\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        self.layers = OrderedDict(reversed(list(self.layers.items())))\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropLayerNet :\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['D1'] = DropLayer()\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['D2'] = DropLayer()\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    \n",
    "    def scoreFunction(self, x, k1, k2):\n",
    "        x = self.layers['L1'].forward(x)\n",
    "        x = self.layers['D1'].forward(x,k1)\n",
    "        x = self.layers['SiLU1'].forward(x)\n",
    "        x = self.layers['L2'].forward(x)\n",
    "        x = self.layers['D2'].forward(x,k2)\n",
    "        x = self.layers['SiLU2'].forward(x)\n",
    "        x = self.layers['L3'].forward(x)\n",
    "        score = x\n",
    "        return score\n",
    "    \n",
    "    def drop_forward(self, x, label,k1,k2):\n",
    "        x = self.layers['L1'].forward(x)\n",
    "        x = self.layers['D1'].forward(x,k1)\n",
    "        x = self.layers['SiLU1'].forward(x)\n",
    "        x = self.layers['L2'].forward(x)\n",
    "        x = self.layers['D2'].forward(x,k2)\n",
    "        x = self.layers['SiLU2'].forward(x)\n",
    "        x = self.layers['L3'].forward(x)\n",
    "        score = x\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label, k1, k2):\n",
    "        \n",
    "        score = self.scoreFunction(x, k1, k2)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy  = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backpropagation(self, x, label):\n",
    "        dx = self.lastLayer.backward()\n",
    "        self.layers = OrderedDict(reversed(list(self.layers.items())))\n",
    "        for backlayer in self.layers.values():\n",
    "            dx = backlayer.backward(dx)\n",
    "        #백워드 함수 작성 스코어펑션을 참고하세요\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        self.layers = OrderedDict(reversed(list(self.layers.items())))\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchOptimization(dataset, ThreeLayerNet, learning_rate, epoch=1000):\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    Loss_list = []\n",
    "    for i in range(epoch+1):\n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'],dataset['one_hot_train'])\n",
    "        grads = ThreeLayerNet.backpropagation(dataset['train_X'],dataset['one_hot_train'])\n",
    "        ThreeLayerNet.gradientdescent(grads,learning_rate)\n",
    "        #코드 작성\n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)        \n",
    "   \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_Optimization(dataset, ThreeLayerNet, learning_rate, epoch=100, batch_size=100):    \n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    Loss_list = []\n",
    "    np.random.seed(5)\n",
    "    for i in range(epoch+1):\n",
    "        s= np.arange(dataset['train_X'].shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        dataset['train_X'] = dataset['train_X'][s]\n",
    "        dataset['one_hot_train'] = dataset['one_hot_train'][s]\n",
    "        \n",
    "        for j in range(int(dataset['train_X'].shape[0] / batch_size)):\n",
    "            first_idx = j*batch_size\n",
    "            Loss = ThreeLayerNet.forward(dataset['train_X'][first_idx:first_idx+batch_size,:],dataset['one_hot_train'][first_idx:first_idx+batch_size,:])\n",
    "            grads = ThreeLayerNet.backpropagation(dataset['train_X'][first_idx:first_idx+batch_size,:],dataset['one_hot_train'][first_idx:first_idx+batch_size,:])\n",
    "            ThreeLayerNet.gradientdescent(grads,learning_rate)\n",
    "        # 코드 작성\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "\n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_use_Optimizer(dataset, DropLayerNet, learning_rate, epoch, kill_n_h1 = 0.25, kill_n_h2 = 0.15):\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    Loss_list = []\n",
    "    for i in range(epoch+1):\n",
    "        Loss = DropLayerNet.drop_forward(dataset['train_X'],dataset['one_hot_train'], kill_n_h1, kill_n_h2)\n",
    "        grads = DropLayerNet.backpropagation(dataset['train_X'],dataset['one_hot_train'])\n",
    "        DropLayerNet.gradientdescent(grads,learning_rate)\n",
    "        #코드 작성\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = DropLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'], kill_n_h1, kill_n_h2)\n",
    "            test_acc = DropLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'], kill_n_h1, kill_n_h2)\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "    return DropLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#과제 채점을 위한 세팅\n",
    "train_X, train_label, test_X, test_label = train_test_split(mnist)\n",
    "\n",
    "one_hot_train = one_hot_encoding(train_label)\n",
    "one_hot_test = one_hot_encoding(test_label)\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_X'] = train_X\n",
    "dataset['test_X'] = test_X\n",
    "dataset['one_hot_train'] = one_hot_train\n",
    "dataset['one_hot_test'] = one_hot_test\n",
    "\n",
    "neournlist = [784, 60, 30, 10]\n",
    "\n",
    "TNN_batchOptimizer = ThreeLayerNet(neournlist)\n",
    "TNN_minibatchOptimizer = copy.deepcopy(TNN_batchOptimizer)\n",
    "TNN_dropout = DropLayerNet(neournlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- BATCH --------------------\n",
      "-------------------- MINI_BATCH --------------------\n",
      "[[False  True False ... False  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True False ...  True  True False]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " ...\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True False  True ...  True False  True]]\n",
      "[[ True False  True ...  True  True False]\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True  True False ...  True  True False]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True False  True]]\n",
      "[[ True  True False ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " ...\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True False  True ... False  True  True]\n",
      " [ True  True  True ...  True False  True]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True False]\n",
      " [ True  True  True ...  True  True False]\n",
      " [False  True  True ... False  True  True]]\n",
      "0 \t번째 Loss =  2.3293901072622063\n",
      "0 \t번째 Train_Accuracy :  0.10575\n",
      "0 \t번째 Test_Accuracy :  0.0975\n",
      "[[False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True False ...  True False False]\n",
      " [False False False ...  True False  True]]\n",
      "[[ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True False]\n",
      " [False False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True False  True ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True False  True ...  True False False]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True False ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True False]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True False]\n",
      " ...\n",
      " [False  True False ... False  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True False  True ... False  True False]\n",
      " [ True False  True ...  True  True False]\n",
      " [False  True False ...  True  True False]\n",
      " ...\n",
      " [ True  True  True ...  True  True False]\n",
      " [False  True False ...  True  True False]\n",
      " [ True  True  True ...  True False  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[False  True  True ...  True False False]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True False  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [False False  True ...  True  True False]]\n",
      "[[ True  True  True ...  True False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True False]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True False  True ... False  True  True]\n",
      " ...\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True False  True ... False  True False]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [False  True  True ...  True  True False]\n",
      " [ True  True  True ...  True  True False]\n",
      " [ True  True  True ...  True  True False]]\n",
      "[[ True  True  True ... False  True  True]\n",
      " [False  True  True ...  True  True False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True False ...  True  True  True]\n",
      " [False False  True ...  True False  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[False  True  True ... False False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False False  True]\n",
      " [False  True False ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [False  True  True ... False  True  True]\n",
      " [False  True False ...  True  True  True]\n",
      " ...\n",
      " [False False  True ... False  True False]\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True False  True ...  True False  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " ...\n",
      " [ True  True False ... False  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True False  True ...  True  True False]]\n",
      "[[ True False False ... False  True  True]\n",
      " [ True  True False ...  True False  True]\n",
      " [ True  True False ...  True False  True]\n",
      " ...\n",
      " [False False  True ...  True False False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True False]]\n",
      "[[ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]]\n",
      "[[ True  True  True ... False  True False]\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " ...\n",
      " [False  True  True ... False  True  True]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False  True  True]]\n",
      "[[ True False  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True False]\n",
      " [ True False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[False False False ... False  True  True]\n",
      " [ True  True False ...  True  True False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True False ...  True  True False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True False  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True False]\n",
      " [ True  True False ...  True False  True]\n",
      " ...\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True False  True]\n",
      " [ True False  True ...  True  True False]\n",
      " ...\n",
      " [ True  True False ...  True False  True]\n",
      " [False  True False ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True False]\n",
      " [ True  True False ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True False ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True  True  True ... False  True  True]\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True False  True ... False  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True False]\n",
      " [ True  True False ...  True  True False]\n",
      " [ True  True False ... False  True  True]]\n",
      "[[ True  True  True ...  True False  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]]\n",
      "10 \t번째 Loss =  2.1938668367767344\n",
      "10 \t번째 Train_Accuracy :  0.245625\n",
      "10 \t번째 Test_Accuracy :  0.227\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [False  True  True ... False False  True]\n",
      " ...\n",
      " [ True False  True ...  True  True False]\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True  True  True ... False  True False]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True False  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True False ... False  True False]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ... False False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True False False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True False]]\n",
      "[[False  True  True ...  True False  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [False  True  True ... False  True  True]\n",
      " ...\n",
      " [ True  True False ... False  True  True]\n",
      " [ True  True False ...  True False  True]\n",
      " [ True  True  True ... False  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [False False  True ...  True  True False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[ True False  True ...  True False False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True False False]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True False  True ...  True  True False]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True False  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True False ... False  True  True]]\n",
      "[[ True False  True ...  True False  True]\n",
      " [False  True False ...  True  True False]\n",
      " [ True  True False ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False False  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]]\n",
      "[[ True False False ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " ...\n",
      " [False  True False ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True False  True ... False  True  True]\n",
      " [False  True  True ...  True  True False]\n",
      " ...\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True  True  True ...  True False False]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ... False  True  True]\n",
      " ...\n",
      " [ True  True  True ... False False  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True  True  True ...  True False  True]]\n",
      "[[ True  True  True ...  True False False]\n",
      " [ True False  True ...  True False  True]\n",
      " [False False False ...  True  True  True]\n",
      " ...\n",
      " [ True False False ...  True  True  True]\n",
      " [ True  True  True ...  True  True False]\n",
      " [False  True  True ...  True  True  True]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [ True False  True ... False False  True]\n",
      " [ True  True  True ... False False  True]\n",
      " ...\n",
      " [ True False  True ...  True  True  True]\n",
      " [ True  True  True ...  True False  True]\n",
      " [False  True  True ...  True False  True]]\n",
      "[[ True  True  True ...  True  True False]\n",
      " [ True  True  True ...  True False  True]\n",
      " [ True False False ...  True  True False]\n",
      " ...\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [False  True False ...  True  True False]]\n",
      "[[False  True  True ...  True  True  True]\n",
      " [False  True  True ...  True False  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True False  True ...  True  True  True]\n",
      " [False  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True False]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a43d91429b40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MINI_BATCH'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#trained_minibatch, tmb_train_acc_list, tmb_test_acc_list, tb_loss_list = minibatch_Optimization(dataset, TNN_minibatchOptimizer, 0.1, epoch=100, batch_size=100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrained_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_train_acc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_test_acc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout_use_Optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTNN_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-e680b6a04271>\u001b[0m in \u001b[0;36mdropout_use_Optimizer\u001b[1;34m(dataset, DropLayerNet, learning_rate, epoch, kill_n_h1, kill_n_h2)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mLoss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropLayerNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'one_hot_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkill_n_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkill_n_h2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropLayerNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'one_hot_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mDropLayerNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradientdescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-43c71389f6fa>\u001b[0m in \u001b[0;36mdrop_forward\u001b[1;34m(self, x, label, k1, k2)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdrop_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'L1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'D1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SiLU1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ffbceac8949d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#       Z(결과값) = 입력받은 x값에 weight를 곱하고 bias를 더하여 계산한다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#내적연산을 통한 Z값 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#채점은 이 것의 결과값으로 할 예정입니다. \n",
    "print(20*'-','BATCH',20*'-')\n",
    "# trained_batch, tb_train_acc_list, tb_test_acc_list, tb_loss_list =  batchOptimization(dataset, TNN_batchOptimizer, 0.1, 1000)\n",
    "print(20*'-','MINI_BATCH',20*'-')\n",
    "#trained_minibatch, tmb_train_acc_list, tmb_test_acc_list, tb_loss_list = minibatch_Optimization(dataset, TNN_minibatchOptimizer, 0.1, epoch=100, batch_size=100)\n",
    "trained_dropout, td_train_acc_list, td_test_acc_list, td_loss_list = dropout_use_Optimizer(dataset, TNN_dropout, 0.1, 1000, 0.25, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
